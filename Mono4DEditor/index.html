<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Mono4DEditor: Text-Driven 4D Scene Editing from Monocular Video via Point-Level Localization of Language-Embedded Gaussians">
  <meta name="keywords" content="3D Gaussians, Language Embedded 3D Scene Representation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Mono4DEditor: Text-Driven 4D Scene Editing from Monocular Video via Point-Level Localization of Language-Embedded Gaussians</title>

  <!-- Bootstrap -->
  <link href="static/css/bootstrap-4.4.1.css" rel="stylesheet">
  
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-V2NT8PJYCQ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-V2NT8PJYCQ');
  </script>

  <script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h2 class="title is-2 publication-title" style="margin-top: 0; margin-bottom: 0">Mono4DEditor: Text-Driven 4D Scene Editing from Monocular Video via Point-Level Localization of Language-Embedded Gaussians</h2>
          <br/>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://chuan-10.github.io/">Jin-Chuan Shi</a><sup> 1, 2 * </sup>
            </span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block">
              Chenye Su<sup> 2 * </sup>
            </span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block">
              Jiajun Wang<sup> 2 </sup>
            </span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block">
              <a href="https://faculty.runi.ac.il/arik/site/index.asp">Ariel Shamir</a><sup> 3 </sup>
            </span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block">
              <a href="http://miaowang.me/">Miao Wang</a><sup> 2 </sup>
            </span>&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup> Zhejiang University
            <span class="author-block"><sup>2</sup> State Key Laboratory of Virtual Reality Technology and Systems, Beihang University
            <span class="author-block"><sup>3</sup> Reichman University

          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup> Equal contribution
          </div>
          
          <!-- <h2 class="title is-5 publication-title" style="color:#6e6e6e;margin-top: 2; margin-bottom: 2">CVPR 2024</h2> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a class="external-link button is-normal is-rounded is-dark" href="https://arxiv.org/abs/2311.18482">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a class="external-link button is-normal is-rounded is-dark disabled" href="https://github.com/buaavrcg/Mono4DEditor">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!--
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./resources/teaser.png"
                type="video/mp4">
      </video>
      -->
      <img src="./resources/teaser.png" class="center" style="max-width: 100%; height: auto; display: block; margin: 0 auto;">
      <h2 class="subtitle has-text-centered" style="margin-top: 15px">
        TL;DR: xxx. 
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="margin-top: -30px">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Editing 4D scenes reconstructed from monocular videos based on text prompts is a valuable yet challenging task with broad applications in content creation and virtual environments. The key difficulty lies in achieving semantically precise edits in localized regions of complex, dynamic scenes, while preserving the integrity of unedited content. 
            To address this, we introduce <b>Mono4DEditor</b>, a novel framework for flexible and accurate text-driven 4D scene editing. Our method augments 3D Gaussians with quantized CLIP features to form a language-embedded dynamic representation, enabling efficient semantic querying of arbitrary spatial regions. We further propose a two-stage point-level localization strategy that first selects candidate Gaussians via CLIP similarity and then refines their spatial extent to improve accuracy. Finally, targeted edits are performed on localized regions using a diffusion-based video editing model, with flow and scribble guidance ensuring spatial fidelity and temporal coherence. Extensive experiments demonstrate that Mono4DEditor enables high-quality, text-driven edits across diverse scenes and object types, while preserving the appearance and geometry of unedited areas and surpassing prior approaches in both flexibility and visual fidelity.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    <!-- Paper video. -->
    <!--
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    -->
    <!--/ Paper video. -->
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" style="margin-top: -20px">Overview</h2>

        <img src="./resources/overview.png" class="center" width="100%">
        <div class="content has-text-justified">
          <p style="margin-top: 30px">
            The training process for Language-embedded 3D Gaussians starts with initializing scenes following 3D Gaussian Splatting and randomly initializing semantic features and setting uncertainty to zero. Dense language features from multi-view CLIP and DINO are quantized to create a discrete feature space and semantic indices. These attributes of the 3D Gaussians are then rendered into 2D maps using a differentiable rasterizer. The optimization is achieved through semantic and adaptive spatial smoothing loss.
          </p>
        </div>

      </div>
    </div>

  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">


    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: -30px">Results</h2>

        <div class="content has-text-justified">
          <p>
            Quantitative comparison of our method with DFF, LeRF, 3DOVS. 
            The table presents a comparison across various metrics, including novel view synthesis quality, open-vocabulary query accuracy, and computational efficiency. We report both host memory and video memory usage, as well as the disk space used for storing the learned language features. Our approach outperforms others in rendering quality and semantic query accuracy, while also offering lower computational demands and a significant speed increase, nearly 100 times faster in inference.
            <br>
          </p>
        </div>
    
        <div class="content has-text-centered">
          <img src="./resources/table.png" class="center" width="100%">
        </div>    

        <div class="content has-text-justified">
          <p>
            Comparison of novel view synthesis and query relevance visualization. Left to right: Ground truth novel view synthesis, novel view images with relevance visualization from our method, DFF, LeRF, and 3DOVS. Top to bottom: Query words ``asphalt ground'', ``bicycle'', ``jar of coconut oil'', ``flower'', ``LEGO Technic 856 Bulldozer'', and ``brown shoes''.
            <br>
          </p>
        </div>

        <div class="content has-text-centered">
          <img src="./resources/comp.png" class="center" width="100%">
        </div>

        <div class="content has-text-justified">
          <p>
            Examples of various open-vocabulary queries. 
            Our approach enables accurate open-vocabulary queries using a diverse class of word types, including but not limited to, visual attributes, general terms, materials, olfactory properties, and related actions.
            <br>
          </p>
        </div>

        <div class="content has-text-centered">
          <img src="./resources/open.png" class="center" width="100%">
        </div>

      </div>
    </div>

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: -30px">Video Results</h2>

        <div class="item">
          <video id="replay-video"
                autoplay
                controls
                muted
                preload
                playsinline
                loop
                width="100%"
                height="100%">
            <source src="./resources/bicycle.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video id="replay-video"
                autoplay
                controls
                muted
                preload
                playsinline
                loop
                width="100%"
                height="100%">
            <source src="./resources/garden.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video id="replay-video"
                autoplay
                controls
                muted
                preload
                playsinline
                loop
                width="100%"
                height="100%">
            <source src="./resources/counter.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video id="replay-video"
                autoplay
                controls
                muted
                preload
                playsinline
                loop
                width="100%"
                height="100%">
            <source src="./resources/room.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video id="replay-video"
                autoplay
                controls
                muted
                preload
                playsinline
                loop
                width="100%"
                height="100%">
            <source src="./resources/bonsai.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video id="replay-video"
                autoplay
                controls
                muted
                preload
                playsinline
                loop
                width="100%"
                height="100%">
            <source src="./resources/kitchen.mp4" type="video/mp4">
          </video>

      </div>
    </div>

  </div>
</section>

<br>
<section class="section" id="Related work">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Related links</h2>
    We recognize that a few concurrent works address similar problems to ours. We encourage you to check them out: <br>
    <li><a href="https://www.cis.upenn.edu/~leijh/projects/mosca/">MoSca: Dynamic Gaussian Fusion from Casual Videos via 4D Motion Scaffolds</a></li>
    <li><a href="https://arxiv.org/pdf/2406.18717">Dynamic Gaussian Marbles for Novel View Synthesis of Casual Monocular Videos</a></li>
    <li><a href="https://dreamscene4d.github.io">DreamScene4D: Dynamic Multi-Object Scene Generation from Monocular Videos</a></li> 
    <li><a href="https://littlepure2333.github.io/GFlow/">GFlow: Recovering 4D World from Monocular Video</a></li>
    <li><a href="https://modgs.github.io/">MoDGS: Dynamic Gaussian Splatting from Causually-captured Monocular Videos</a></li>
  </div>
</section>

<br>
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    
    <div class="content has-text-justified">
      <p>
        If you find this work useful for your work, please cite us:
      </p>
    </div>

    <pre><code>
      xxx
    </code></pre>
  </div>
</section>

<br>
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
            This webpage template is from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. 
            We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template.
          </p>
        </div>
      </div>
          <p></p>
        </div>
      </div>
</footer>

</body>
</html>
